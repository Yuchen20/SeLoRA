{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7rj7vqILo_d"
      },
      "outputs": [],
      "source": [
        "import os, gc, sys, time, random, math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from typing import Optional, List\n",
        "\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "except:\n",
        "    !pip install diffusers -q\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "##### Setting Random Seed\n",
        "DEFAULT_RANDOM_SEED = 42\n",
        "\n",
        "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "# torch random seed\n",
        "def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seedBasic(); seedTorch();\n",
        "\n",
        "def clear_cache():\n",
        "    torch.cuda.empty_cache(); gc.collect(); time.sleep(1); torch.cuda.empty_cache(); gc.collect();\n",
        "\n",
        "\n",
        "def check_and_make_folder(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okivtuSjQD9Z"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    total_parameter_count = sum([np.prod(p.size()) for p in model.parameters()])\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.lora_A_temp.requires_grad = False\n",
        "            layer.lora_B_temp.requires_grad = False\n",
        "            total_parameter_count -= np.prod(layer.lora_A_temp.size()) + np.prod(layer.lora_B_temp.size())\n",
        "\n",
        "    trainable_parameter = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    trainable_parameter_count = sum([np.prod(p.size()) for p in trainable_parameter])\n",
        "\n",
        "    trainable_percentage = (trainable_parameter_count / total_parameter_count)  * 100\n",
        "\n",
        "    formatted_output = (\n",
        "        f\"trainable params: {trainable_parameter_count:,} || \"\n",
        "        f\"all params: {total_parameter_count:,} || \"\n",
        "        f\"trainable%: {trainable_percentage:.16f}\"\n",
        "    )\n",
        "\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.lora_A_temp.requires_grad = True\n",
        "            layer.lora_B_temp.requires_grad = True\n",
        "\n",
        "    print(formatted_output)\n",
        "\n",
        "\n",
        "def remove_param_from_optimizer(optim, param):\n",
        "    for j in range(len(optim.param_groups)):\n",
        "        optim_param_group_list = optim.param_groups[j][\"params\"]\n",
        "        for i, optim_param in enumerate(optim_param_group_list):\n",
        "            if param.shape == optim_param.shape and (param==optim_param).all():\n",
        "                del optim.param_groups[j][\"params\"][i]\n",
        "\n",
        "clear_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and Path"
      ],
      "metadata": {
        "id": "30MLotwUV-vq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs2wZlnKLo_e"
      },
      "outputs": [],
      "source": [
        "WEIGHT_DTYPE = torch.float32\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "# The main path\n",
        "Main_Path = ''\n",
        "# The datasets path under the main path\n",
        "Data_storage = Main_Path + '/Dataset/IU chest'\n",
        "save_result_path = Main_Path + '/Result'\n",
        "\n",
        "### folder to save the result.\n",
        "folder_name = 'SE LoRA'\n",
        "\n",
        "assert not os.path.exists(f'{save_result_path}/{folder_name}'), print('LoRA Experiment Already Run')\n",
        "\n",
        "check_and_make_folder(f'{save_result_path}/{folder_name}')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "UNET_TARGET_MODULES = [\n",
        "    \"to_q\", \"to_k\", \"to_v\",\n",
        "    \"proj\", \"proj_in\", \"proj_out\",\n",
        "    \"conv\", \"conv1\", \"conv2\",\n",
        "    \"conv_shortcut\", \"to_out.0\", \"time_emb_proj\", \"ff.net.2\",\n",
        "]\n",
        "\n",
        "\n",
        "TEXT_ENCODER_TARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFmKYAasExHh"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "tokenizer = pipe.tokenizer\n",
        "noise_scheduler = pipe.scheduler\n",
        "text_encoder = pipe.text_encoder\n",
        "vae = pipe.vae\n",
        "unet = pipe.unet\n",
        "\n",
        "# Freeze the Bulk part of the model\n",
        "text_encoder.requires_grad_(False)\n",
        "vae.requires_grad_(False)\n",
        "unet.requires_grad_(False)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CnZ5-d2zKmt"
      },
      "outputs": [],
      "source": [
        "def remove_param_from_optimizer(optim, param):\n",
        "    for j in range(len(optim.param_groups)):\n",
        "        optim_param_group_list = optim.param_groups[j][\"params\"]\n",
        "        for i, optim_param in enumerate(optim_param_group_list):\n",
        "            if param.shape == optim_param.shape and (param==optim_param).all():\n",
        "                del optim.param_groups[j][\"params\"][i]\n",
        "\n",
        "clear_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SeLoRA class\n",
        "Below Defines the SeLoRA injected linear layer class, shown in `Linear`."
      ],
      "metadata": {
        "id": "mOmMl4K6MC1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTYpP8zhy6TL"
      },
      "outputs": [],
      "source": [
        "class LoRALayer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        r: int,\n",
        "        lora_alpha: int,\n",
        "        lora_dropout: float,\n",
        "        merge_weights: bool,\n",
        "    ):\n",
        "        self.r = r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        # Optional dropout\n",
        "        if lora_dropout > 0.:\n",
        "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
        "        else:\n",
        "            self.lora_dropout = lambda x: x\n",
        "        # Mark the weight as unmerged\n",
        "        self.merged = False\n",
        "        self.merge_weights = merge_weights\n",
        "\n",
        "### This is the SeLoRA replacement of nn.Linear layer\n",
        "class Linear(nn.Linear, LoRALayer):\n",
        "    # LoRA implemented in a dense layer\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        r: int = 0,\n",
        "        lora_alpha: int = 8,\n",
        "        lora_dropout: float = 0.,\n",
        "        EMA_factor: float = 0.6,\n",
        "        # warmup_step_per_expand:int = 10,\n",
        "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
        "        merge_weights: bool = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
        "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
        "                           merge_weights=merge_weights)\n",
        "\n",
        "        self.r = r\n",
        "        self.fan_in_fan_out = fan_in_fan_out\n",
        "        # Actual trainable parameters\n",
        "        if r > 0:\n",
        "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
        "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
        "\n",
        "            self.lora_A_temp = nn.Parameter(self.weight.new_zeros((r + 1, in_features)))\n",
        "            self.lora_B_temp = nn.Parameter(self.weight.new_zeros((out_features, r + 1)))\n",
        "\n",
        "            self.use_temp_weight = False\n",
        "\n",
        "            self.scaling = self.lora_alpha / self.r\n",
        "            # Freezing the pre-trained weight matrix\n",
        "            self.weight.requires_grad = False\n",
        "\n",
        "            self.recorded_grad = 1\n",
        "\n",
        "        self.reset_parameters()\n",
        "        if fan_in_fan_out:\n",
        "            self.weight.data = self.weight.data.T\n",
        "\n",
        "    def get_active_rank(self):\n",
        "        assert self.lora_A.shape[0] == self.lora_B.shape[1]\n",
        "        return self.lora_A.shape[0]\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.Linear.reset_parameters(self)\n",
        "        if hasattr(self, 'lora_A'):\n",
        "            nn.init.zeros_(self.lora_B)\n",
        "            nn.init.kaiming_uniform_(self.lora_A)\n",
        "            nn.init.zeros_(self.lora_B_temp)\n",
        "            nn.init.kaiming_uniform_(self.lora_A)\n",
        "\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        def T(w):\n",
        "            return w.T if self.fan_in_fan_out else w\n",
        "        nn.Linear.train(self, mode)\n",
        "        if self.merge_weights and self.merged:\n",
        "            # Make sure that the weights are not merged\n",
        "            if self.r > 0:\n",
        "                self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
        "            self.merged = False\n",
        "\n",
        "    def eval(self):\n",
        "        def T(w):\n",
        "            return w.T if self.fan_in_fan_out else w\n",
        "        nn.Linear.eval(self)\n",
        "        if self.merge_weights and not self.merged:\n",
        "            # Merge the weights and mark it\n",
        "            if self.r > 0:\n",
        "\n",
        "                self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
        "\n",
        "            self.merged = True\n",
        "\n",
        "    def forward(self, x: torch.Tensor,*args,  **kwargs):\n",
        "        def T(w):\n",
        "            return w.T if self.fan_in_fan_out else w\n",
        "\n",
        "        if self.r > 0 and not self.merged:\n",
        "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
        "\n",
        "            if self.r > 0:\n",
        "                if not self.use_temp_weight:\n",
        "                    result += (self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
        "                else:\n",
        "                    self.lora_A_temp.data[:-1, :] = self.lora_A.data\n",
        "                    self.lora_B_temp.data[:, :-1] = self.lora_B.data\n",
        "\n",
        "                    result += (self.lora_dropout(x) @ self.lora_A_temp.T @ self.lora_B_temp.T) * self.scaling\n",
        "            return result\n",
        "        else:\n",
        "            return F.linear(x, T(self.weight), bias=self.bias)\n",
        "\n",
        "    def sum_square(self, _A, _B):\n",
        "        return torch.sum(torch.square(_A)) + torch.sum(torch.square(_B))\n",
        "\n",
        "\n",
        "    def get_rmse_grad(self):\n",
        "        if not self.use_temp_weight:\n",
        "            return self.sum_square(self.lora_A.grad, self.lora_B.grad)\n",
        "        else:\n",
        "            return self.sum_square(self.lora_A_temp.grad, self.lora_B_temp.grad)\n",
        "\n",
        "    def change_to_temp(self):\n",
        "        if not self.use_temp_weight:\n",
        "            self.recorded_grad = self.get_rmse_grad()\n",
        "            self.use_temp_weight = True\n",
        "            self.lora_A.requires_grad = False\n",
        "            self.lora_B.requires_grad = False\n",
        "            self.lora_A_temp.requires_grad = True\n",
        "            self.lora_B_temp.requires_grad = True\n",
        "        else:\n",
        "            self.use_temp_weight = False\n",
        "            self.lora_A.requires_grad = True\n",
        "            self.lora_B.requires_grad = True\n",
        "            self.lora_A_temp.requires_grad = False\n",
        "            self.lora_B_temp.requires_grad = False\n",
        "\n",
        "\n",
        "    def get_ratio(self):\n",
        "        if not self.use_temp_weight:\n",
        "            return 0\n",
        "        return self.get_rmse_grad() / self.recorded_grad\n",
        "\n",
        "\n",
        "\n",
        "    def expand_rank(self, optimizer):\n",
        "\n",
        "        old_lora_A = self.lora_A.data\n",
        "        remove_param_from_optimizer(optimizer, self.lora_A)\n",
        "        self.lora_A = nn.Parameter(self.weight.new_zeros((self.lora_A.shape[0] + 1, self.lora_A.shape[1])))\n",
        "        nn.init.kaiming_uniform_(self.lora_A)\n",
        "        self.lora_A.data[:-1, :] = old_lora_A\n",
        "\n",
        "\n",
        "        old_lora_B = self.lora_B.data\n",
        "        remove_param_from_optimizer(optimizer, self.lora_B)\n",
        "        self.lora_B = nn.Parameter(self.weight.new_zeros((self.lora_B.shape[0], self.lora_B.shape[1] + 1)))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "        self.lora_B.data[:, :-1] = old_lora_B\n",
        "\n",
        "\n",
        "\n",
        "        remove_param_from_optimizer(optimizer, self.lora_A_temp)\n",
        "        remove_param_from_optimizer(optimizer, self.lora_B_temp)\n",
        "        self.lora_A_temp = nn.Parameter(self.weight.new_zeros((self.lora_A.shape[0] + 1, self.lora_A.shape[1])))\n",
        "        nn.init.kaiming_uniform_(self.lora_A_temp)\n",
        "        self.lora_B_temp = nn.Parameter(self.weight.new_zeros((self.lora_B.shape[0], self.lora_B.shape[1] + 1)))\n",
        "        nn.init.zeros_(self.lora_B_temp)\n",
        "\n",
        "        optimizer.add_param_group({'params': self.lora_A})\n",
        "        optimizer.add_param_group({'params': self.lora_B})\n",
        "        optimizer.add_param_group({'params': self.lora_A_temp})\n",
        "        optimizer.add_param_group({'params': self.lora_B_temp})\n",
        "\n",
        "\n",
        "\n",
        "        return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyK_0IalHdHL",
        "outputId": "56534bfa-4e06-4ee3-e87f-fba38c5002d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 165,888 || all params: 123,226,368 || trainable%: 0.1346205383575048\n",
            "trainable params: 310,016 || all params: 859,830,980 || trainable%: 0.0360554582483176\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_encoder = pipe.text_encoder\n",
        "text_encoder = text_encoder.requires_grad_(False)\n",
        "unet = pipe.unet\n",
        "unet = unet.requires_grad_(False)\n",
        "\n",
        "def set_Linear_SeLoRA(model, target_modules):\n",
        "     # replace all linear layer (include q,k,v layer) into DyLoRA Layer.\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, nn.Linear):\n",
        "\n",
        "            LoRA_layer = Linear(\n",
        "                  in_features = layer.in_features,\n",
        "                  out_features = layer.out_features,\n",
        "                  r = 1\n",
        "            )\n",
        "\n",
        "            LoRA_layer.weight = layer.weight\n",
        "            LoRA_layer.weight.requires_grad = False\n",
        "            LoRA_layer.bias = layer.bias\n",
        "            if LoRA_layer.bias != None:\n",
        "                LoRA_layer.bias.requires_grad = False\n",
        "\n",
        "            pointing_layer = model\n",
        "            if len(target_modules) != len(None)\n",
        "                if name.split('.')[-1] in target_modules:\n",
        "                    for layer_name in name.split('.')[:-1]:\n",
        "                        pointing_layer = getattr(pointing_layer, layer_name)\n",
        "            else:\n",
        "                for layer_name in name.split('.')[:-1]:\n",
        "                        pointing_layer = getattr(pointing_layer, layer_name)\n",
        "\n",
        "                setattr(pointing_layer, name.split('.')[-1], LoRA_layer)\n",
        "    return model\n",
        "\n",
        "\n",
        "# unet_lora = unet\n",
        "unet_lora = set_Linear_SeLoRA(unet, UNET_TARGET_MODULES)\n",
        "text_encoder_lora = set_Linear_SeLoRA(text_encoder, TEXT_ENCODER_TARGET_MODULES)\n",
        "\n",
        "print_trainable_parameters(text_encoder_lora)\n",
        "print_trainable_parameters(unet_lora)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets\n",
        "\n",
        "The dataset is designed for IU-Xray.\n",
        "Image is stored und"
      ],
      "metadata": {
        "id": "nznBHGLwVpV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, tokenizer, size = 224, center_crop = True):\n",
        "        self.root_dir = root_dir\n",
        "        self.files = df['filename'].tolist()\n",
        "        self.findings = df['findings'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = {}\n",
        "        instance_image = Image.open(\n",
        "            os.path.join(self.root_dir, self.files[idx])\n",
        "        ).convert(\"RGB\")\n",
        "\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.findings[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids\n",
        "\n",
        "        return example\n"
      ],
      "metadata": {
        "id": "7P3hL8jQaMhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDnjtIN4Lo_f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "reports = reports[['filename', 'findings']]\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(reports, test_size=0.2, random_state=42)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# ImageDataset initialization\n",
        "train_ds = ImageDataset(\n",
        "    root_dir=os.path.join(Data_storage, 'images/images'),\n",
        "    df=train_df,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "valid_ds = ImageDataset(\n",
        "    root_dir=os.path.join(Data_storage, 'images/images'),\n",
        "    df=valid_df,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "test_ds = ImageDataset(\n",
        "    root_dir=os.path.join(Data_storage, 'images/images'),\n",
        "    df=test_df,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "num_workers = os.cpu_count()\n",
        "\n",
        "# DataLoader initialization\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers = num_workers)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE // 2, shuffle=False, num_workers = num_workers)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE // 2, shuffle=False, num_workers = num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig1a4yA_Lo_g"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(list(unet_lora.parameters()) + list(text_encoder_lora.parameters()), lr=LR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "uEI8V7LBVnTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXBDrJKo4kuY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display\n",
        "import copy\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, vae, unet, text_encoder, noise_scheduler, optimizer, train_dl, test_dl, total_epoch, WEIGHT_DTYPE, threshould = 2, per_iter_valid = 60, log_period = 20, expand_step = 20):\n",
        "        self.vae = vae.to(device, dtype=WEIGHT_DTYPE)\n",
        "        self.unet = unet.to(device, dtype=WEIGHT_DTYPE)\n",
        "        self.text_encoder = text_encoder.to(device, dtype=WEIGHT_DTYPE)\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dl = train_dl\n",
        "        self.test_dl = test_dl\n",
        "        self.WEIGHT_DTYPE = WEIGHT_DTYPE\n",
        "        self.total_epoch = total_epoch\n",
        "        self.threshould = threshould\n",
        "        self.per_iter_valid = per_iter_valid\n",
        "        self.total_step = 0\n",
        "        self.result_df = pd.DataFrame(columns=['epoch', 'steps', 'Train Loss', 'Valid Loss', 'Total Added Rank', 'unet trainable', 'text_encoder trainable'])\n",
        "        self._display_id = None\n",
        "        self.log_period = log_period\n",
        "        self.expand_step = expand_step\n",
        "\n",
        "        self.best_text_encoder = None\n",
        "        self.best_unet = None\n",
        "\n",
        "        self.display_line = ''\n",
        "\n",
        "        self.added_rank = 0\n",
        "\n",
        "        print(f'total steps: {len(train_dl) * total_epoch}')\n",
        "\n",
        "\n",
        "\n",
        "    def Expandable_LoRA(self, model):\n",
        "        for name, layer in model.named_modules():\n",
        "            if isinstance(layer, Linear):\n",
        "                self.display_line += f'{layer.get_ratio():.4f}, {layer.get_active_rank()}   '\n",
        "                if layer.get_ratio() >= self.threshould:\n",
        "                    self.added_rank += 1\n",
        "                    self.optimizer = layer.expand_rank(self.optimizer)\n",
        "        self.rank_display_id.update(self.display_line)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def valid(self):\n",
        "        self.unet.eval()\n",
        "        self.text_encoder.eval()\n",
        "        self.vae.eval()\n",
        "\n",
        "        valid_pbar = tqdm(self.test_dl, desc = 'validating', leave = False)\n",
        "\n",
        "        valid_loss, number_of_instance = [], 0\n",
        "\n",
        "        for step, batch in enumerate(valid_pbar):\n",
        "\n",
        "            pixel_values = batch[\"instance_images\"].to(device, dtype=self.WEIGHT_DTYPE)\n",
        "            pormpt_idxs  = batch[\"instance_prompt_ids\"].to(device).squeeze(1)\n",
        "\n",
        "            # Convert images to latent space\n",
        "            latents = self.vae.encode(pixel_values).latent_dist.sample()\n",
        "            latents = latents * self.vae.config.scaling_factor\n",
        "\n",
        "            # Sample noise that we'll add to the latents\n",
        "            noise = torch.randn_like(latents)\n",
        "            bsz = latents.shape[0]\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "            timesteps = timesteps.long()\n",
        "\n",
        "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "\n",
        "            # Get the text embedding for conditioning\n",
        "            encoder_hidden_states = self.text_encoder(pormpt_idxs)[0]\n",
        "            # Predict the noise residual\n",
        "            model_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "            target = noise\n",
        "\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "            valid_loss.append(loss.item() * len(batch))\n",
        "            number_of_instance += len(batch)\n",
        "            # clear_cache()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        ## add log and save model here TODO\n",
        "        ########################################################################\n",
        "\n",
        "        self.unet.train()\n",
        "        self.vae.train()\n",
        "        self.text_encoder.train()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return sum(valid_loss) / number_of_instance\n",
        "\n",
        "    def trainable_percentage(self, model):\n",
        "\n",
        "        total_parameter_count = sum([np.prod(p.size()) for p in model.parameters()])\n",
        "\n",
        "        trainable_parameter_count = sum([np.prod(p.size()) for p in filter(lambda p: p.requires_grad, model.parameters())])\n",
        "\n",
        "        return (trainable_parameter_count / total_parameter_count)  * 100\n",
        "\n",
        "    def model_to_temp(self, model):\n",
        "        for name, layer in model.named_modules():\n",
        "            if isinstance(layer, Linear):\n",
        "                layer.change_to_temp()\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self._display_id = display(self.result_df, display_id=True)\n",
        "        self.rank_display_id = display('', display_id=True)\n",
        "\n",
        "        self.vae.train()\n",
        "        self.unet.train()\n",
        "        self.text_encoder.train()\n",
        "\n",
        "\n",
        "        recorded_loss = []\n",
        "\n",
        "        for epoch in range(self.total_epoch):\n",
        "\n",
        "            pbar = tqdm(self.train_dl)\n",
        "            for step, batch in enumerate(pbar):\n",
        "\n",
        "                pixel_values = batch[\"instance_images\"].to(device, dtype=self.WEIGHT_DTYPE)\n",
        "                pormpt_idxs  = batch[\"instance_prompt_ids\"].to(device).squeeze(1)\n",
        "\n",
        "                # Convert images to latent space\n",
        "                latents = self.vae.encode(pixel_values).latent_dist.sample()\n",
        "                latents = latents * self.vae.config.scaling_factor\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = self.text_encoder(pormpt_idxs)[0]\n",
        "                # Predict the noise residual\n",
        "                model_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                target = noise\n",
        "\n",
        "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "\n",
        "                recorded_loss.append(loss.item())\n",
        "\n",
        "                pbar.set_description(f\"[Loss: {recorded_loss[-1]:.3f}/{np.mean(recorded_loss):.3f}]\")\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                self.total_step += 1\n",
        "                ########################################################################\n",
        "                ## check number of expandable LoRA\n",
        "                ########################################################################\n",
        "\n",
        "                if self.total_step % self.expand_step == 0:\n",
        "                    self.model_to_temp(self.unet)\n",
        "                    self.model_to_temp(self.text_encoder)\n",
        "\n",
        "                    # Get the text embedding for conditioning\n",
        "                    encoder_hidden_states = self.text_encoder(pormpt_idxs)[0]\n",
        "                    # Predict the noise residual\n",
        "                    model_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "                    loss.backward()\n",
        "\n",
        "\n",
        "                    self.display_line = ''\n",
        "                    self.Expandable_LoRA(self.unet)\n",
        "                    self.Expandable_LoRA(self.text_encoder)\n",
        "\n",
        "\n",
        "                    self.model_to_temp(self.unet)\n",
        "                    self.model_to_temp(self.text_encoder)\n",
        "\n",
        "\n",
        "                ########################################################################\n",
        "\n",
        "\n",
        "                # clear_cache()\n",
        "\n",
        "\n",
        "                if self.total_step % self.per_iter_valid == 0:\n",
        "\n",
        "                    valid_rmse = self.valid()\n",
        "\n",
        "                    if valid_rmse <= min([x for x in trainer.result_df['Valid Loss'] if x != ' --- '] + [1.0]):\n",
        "\n",
        "                        self.best_text_encoder = copy.deepcopy(self.text_encoder).cpu()\n",
        "                        self.best_unet = copy.deepcopy(self.unet).cpu()\n",
        "\n",
        "                        check_and_make_folder(f'{save_result_path}/{folder_name}/trained_model')\n",
        "                        check_and_make_folder(f'{save_result_path}/{folder_name}/trained_model/final_Unet')\n",
        "                        check_and_make_folder(f'{save_result_path}/{folder_name}/trained_model/final_Text')\n",
        "\n",
        "                        self.unet.save_pretrained(f'{save_result_path}/{folder_name}/trained_model/final_Unet')\n",
        "                        self.text_encoder.save_pretrained(f'{save_result_path}/{folder_name}/trained_model/final_Text')\n",
        "\n",
        "\n",
        "                    self.result_df.loc[len(self.result_df)] = [epoch, self.total_step, np.round(np.mean(recorded_loss), 4), np.round(valid_rmse, 4), self.added_rank,  self.trainable_percentage(self.unet), self.trainable_percentage(self.text_encoder)]\n",
        "\n",
        "\n",
        "                    self._display_id.update(self.result_df)\n",
        "\n",
        "                    recorded_loss = []\n",
        "\n",
        "                if self.total_step % self.log_period == 0:\n",
        "\n",
        "                    self.result_df.loc[len(self.result_df)] = [epoch, self.total_step, np.round(np.mean(recorded_loss), 4), ' --- ', self.added_rank,  self.trainable_percentage(self.unet), self.trainable_percentage(self.text_encoder)]\n",
        "\n",
        "                    self._display_id.update(self.result_df)\n",
        "\n",
        "                    self.result_df.to_csv(f'{save_result_path}/{folder_name}/results.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    vae = vae,\n",
        "    unet = unet_lora,\n",
        "    text_encoder = text_encoder_lora,\n",
        "    noise_scheduler = noise_scheduler,\n",
        "    optimizer = optimizer,\n",
        "    train_dl = train_loader,\n",
        "    test_dl = valid_loader,\n",
        "    total_epoch = 10,\n",
        "    WEIGHT_DTYPE = WEIGHT_DTYPE,\n",
        "    threshould = 1.3,\n",
        "    per_iter_valid = len(train_loader),\n",
        "    log_period = 40,\n",
        "    expand_step = 40,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udc7XMc_1UjI"
      },
      "outputs": [],
      "source": [
        "trainer.result_df.to_csv(f'{save_result_path}/{folder_name}/results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing / Inference"
      ],
      "metadata": {
        "id": "KiGfCMJ1VjT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKYiHJexLo_h"
      },
      "outputs": [],
      "source": [
        "unet_lora.eval()\n",
        "text_encoder_lora.eval()\n",
        "clear_cache()\n",
        "new_pipe = StableDiffusionPipeline(\n",
        "    tokenizer=tokenizer,\n",
        "    text_encoder=trainer.best_text_encoder.to(device, dtype=WEIGHT_DTYPE),\n",
        "    vae=vae,\n",
        "    unet=trainer.best_unet.to(device, dtype=WEIGHT_DTYPE),\n",
        "    scheduler=noise_scheduler,\n",
        "    safety_checker= None,\n",
        "    feature_extractor=None\n",
        ")\n",
        "\n",
        "new_pipe.to(device)\n",
        "\n",
        "check_and_make_folder(f'{save_result_path}/{folder_name}')\n",
        "check_and_make_folder(f'{save_result_path}/{folder_name}/test_images')\n",
        "\n",
        "for place in range(len(test_df)):\n",
        "    temp_prompts = test_df.findings.iloc[place]\n",
        "\n",
        "    temp = new_pipe(temp_prompts, height = 224, width = 224).images[0]\n",
        "\n",
        "    temp.save(f'{save_result_path}/{folder_name}/test_images/{place}.png')\n",
        "\n",
        "    display(temp)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 516716,
          "sourceId": 951996,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}